{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8b2f163",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Local\\Temp\\ipykernel_22792\\1188654728.py\", line 1, in <module>\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\__init__.py\", line 84, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\__init__.py\", line 11, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_chunking.py\", line 8, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py\", line 14, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py\", line 26, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_array_api.py\", line 11, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\fixes.py\", line 24, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\ops\\__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\ops\\array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\computation\\expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\computation\\check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\site-packages\\numexpr\\__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: _ARRAY_API not found"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.2 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\tornado\\platform\\asyncio.py\", line 211, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\asyncio\\base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\asyncio\\base_events.py\", line 1986, in _run_once\n",
      "    handle._run()\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\asyncio\\events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 519, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 508, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 400, in dispatch_shell\n",
      "    await result\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 368, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\ipkernel.py\", line 455, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\ipykernel\\zmqshell.py\", line 577, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3116, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3171, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3394, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3639, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\IPython\\core\\interactiveshell.py\", line 3699, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\jburd\\AppData\\Local\\Temp\\ipykernel_22792\\1188654728.py\", line 1, in <module>\n",
      "    from sklearn.ensemble import RandomForestClassifier\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\__init__.py\", line 84, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py\", line 19, in <module>\n",
      "    from .utils._estimator_html_repr import _HTMLDocumentationLinkMixin, estimator_html_repr\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\__init__.py\", line 11, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_chunking.py\", line 8, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_param_validation.py\", line 14, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\validation.py\", line 26, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\_array_api.py\", line 11, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\utils\\fixes.py\", line 24, in <module>\n",
      "    import pandas as pd\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\__init__.py\", line 62, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\arrow\\array.py\", line 64, in <module>\n",
      "    from pandas.core.arrays.masked import BaseMaskedArray\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\arrays\\masked.py\", line 60, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\core\\nanops.py\", line 52, in <module>\n",
      "    bn = import_optional_dependency(\"bottleneck\", errors=\"warn\")\n",
      "  File \"C:\\Users\\jburd\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\compat\\_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\importlib\\__init__.py\", line 90, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"c:\\Users\\jburd\\anaconda3\\Lib\\site-packages\\bottleneck\\__init__.py\", line 7, in <module>\n",
      "    from .move import (move_argmax, move_argmin, move_max, move_mean, move_median,\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\core\\_multiarray_umath.py:44\u001b[39m, in \u001b[36m__getattr__\u001b[39m\u001b[34m(attr_name)\u001b[39m\n\u001b[32m     39\u001b[39m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[32m     43\u001b[39m     sys.stderr.write(msg + tb_msg)\n\u001b[32m---> \u001b[39m\u001b[32m44\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[32m     46\u001b[39m ret = \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mImportError\u001b[39m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.1.2 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "BASE_DIR = r\"C:\\Users\\jburd\\Desktop\\MMASH_1\\DataPaper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d805b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(no):    \n",
    "    user_1_actigraph = pd.read_csv(os.path.join(BASE_DIR, \"user_1\", \"Actigraph.csv\"))\n",
    "    user_1_activity = pd.read_csv(os.path.join(BASE_DIR, \"user_1\", \"Activity.csv\"))\n",
    "    user_1_actigraph.head()\n",
    "    # print(user_1_actigraph.columns)\n",
    "    # print(user_1_activity.columns)\n",
    "    # print(user_1_activity.head(10))\n",
    "    base=pd.Timestamp(\"2020-01-01\")\n",
    "    us1=user_1_actigraph\n",
    "    us1[\"ts\"] = base + pd.to_timedelta(us1[\"day\"] - 1, unit=\"D\") + pd.to_timedelta(us1[\"time\"]) #way to make timestamp unit D is days and default is just time\n",
    "    #creates a 10s window\n",
    "    us1[\"window\"]=us1[\"ts\"].dt.floor(\"10s\")\n",
    "\n",
    "    # print(us1[[\"day\",\"time\",\"ts\",\"window\"]].head(15))\n",
    "    # print(\"Unique Windows:\",us1[\"window\"].nunique)\n",
    "    # print(us1.groupby(\"window\").size().describe())\n",
    "    # Ensure it's sorted by time\n",
    "    us1 = us1.sort_values(\"ts\").reset_index(drop=True) #sorts and drops the old index to make it non sequential\n",
    "    us1_1=user_1_activity\n",
    "    # Build start/end timestamps for intervals\n",
    "    us1_1[\"start_ts\"] = base + pd.to_timedelta(us1_1[\"Day\"] - 1, unit=\"D\") + pd.to_timedelta(us1_1[\"Start\"] + \":00\")\n",
    "    us1_1[\"end_ts\"]   = base + pd.to_timedelta(us1_1[\"Day\"] - 1, unit=\"D\") + pd.to_timedelta(us1_1[\"End\"] + \":00\")\n",
    "\n",
    "    act1 = us1_1.sort_values(\"start_ts\").reset_index(drop=True)\n",
    "    # Attach the most recent interval start <= ts, then drop if ts is after end_ts\n",
    "    us1_labeled = pd.merge_asof(\n",
    "        us1,\n",
    "        act1[[\"start_ts\", \"end_ts\", \"Activity\"]],\n",
    "        left_on=\"ts\",\n",
    "        right_on=\"start_ts\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    us1_labeled[\"Activity\"] = np.where(\n",
    "        us1_labeled[\"end_ts\"].notna() & (us1_labeled[\"ts\"] <= us1_labeled[\"end_ts\"]),  #if condition met keep Activity if not becomes NaN\n",
    "        us1_labeled[\"Activity\"],\n",
    "        np.nan\n",
    "    )\n",
    "    MIN_DOM=0.8 #A 10-second window is kept only if at least 80% of its accelerometer rows correspond to the same activity.\n",
    "    # --- 1) ensure numeric accelerometer columns ---\n",
    "    feat_cols = [\"Axis1\", \"Axis2\", \"Axis3\", \"Vector Magnitude\"]\n",
    "    for c in feat_cols:\n",
    "        us1_labeled[c]=pd.to_numeric(us1_labeled[c],errors=\"coerce\")\n",
    "    # --- 2) compute window-level features: mean + std ---\n",
    "    X_win = us1_labeled.groupby(\"window\")[feat_cols].agg([\"mean\", \"std\"])\n",
    "    X_win.columns = [f\"{a}_{b}\" for a, b in X_win.columns]\n",
    "    X_win = X_win.reset_index()\n",
    "\n",
    "\n",
    "    def dominant_label(s): #s is a column with activity\n",
    "        s=s.dropna()\n",
    "        if s.empty:\n",
    "            return (np.nan,0.0) #return empty list with a dominance of 0\n",
    "        vc = s.value_counts(normalize=True) #proportions if there a different activity durations\n",
    "        return (vc.index[0],float(vc.iloc[0]))\n",
    "\n",
    "    y_win = us1_labeled.groupby(\"window\")[\"Activity\"].apply(dominant_label).reset_index()  #apply the dominant labels\n",
    "    # print(y_win)\n",
    "    y_win[[\"Activity_win\", \"dominance\"]]=pd.DataFrame(y_win[\"Activity\"].to_list(),index=y_win.index)\n",
    "    y_win = y_win.drop(columns=[\"Activity\"])\n",
    "    dataset_user1=X_win.merge(y_win,on=\"window\",how=\"left\") #y_win = y_win.drop(columns=[\"Activity\"])\n",
    "\n",
    "    dataset_user1 = dataset_user1.dropna(subset=[\"Activity_win\"])  \n",
    "    dataset_user1 = dataset_user1[dataset_user1[\"dominance\"] >= MIN_DOM].reset_index(drop=True) #drops the ones that do not have dominance\n",
    "    dataset_user1[\"user\"] = \"user_1\"\n",
    "    return dataset\n",
    "\n",
    "# # Quick checks\n",
    "# print(us1_labeled[[\"ts\", \"window\", \"Activity\"]].head(20))\n",
    "# print(us1_labeled.columns)\n",
    "# print(\"Labeled rows:\", us1_labeled[\"Activity\"].notna().sum(), \"out of\", len(us1_labeled))\n",
    "# print(\"Unique activities (labeled):\", [int(x) for x in sorted(us1_labeled[\"Activity\"].dropna().unique())[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "581db408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    day      time                  ts              window\n",
      "0     1  10:10:22 2020-01-01 10:10:22 2020-01-01 10:10:20\n",
      "1     1  10:10:23 2020-01-01 10:10:23 2020-01-01 10:10:20\n",
      "2     1  10:10:24 2020-01-01 10:10:24 2020-01-01 10:10:20\n",
      "3     1  10:10:25 2020-01-01 10:10:25 2020-01-01 10:10:20\n",
      "4     1  10:10:26 2020-01-01 10:10:26 2020-01-01 10:10:20\n",
      "5     1  10:10:27 2020-01-01 10:10:27 2020-01-01 10:10:20\n",
      "6     1  10:10:28 2020-01-01 10:10:28 2020-01-01 10:10:20\n",
      "7     1  10:10:29 2020-01-01 10:10:29 2020-01-01 10:10:20\n",
      "8     1  10:10:30 2020-01-01 10:10:30 2020-01-01 10:10:30\n",
      "9     1  10:10:31 2020-01-01 10:10:31 2020-01-01 10:10:30\n",
      "10    1  10:10:32 2020-01-01 10:10:32 2020-01-01 10:10:30\n",
      "11    1  10:11:01 2020-01-01 10:11:01 2020-01-01 10:11:00\n",
      "12    1  10:11:02 2020-01-01 10:11:02 2020-01-01 10:11:00\n",
      "13    1  10:11:03 2020-01-01 10:11:03 2020-01-01 10:11:00\n",
      "14    1  10:11:04 2020-01-01 10:11:04 2020-01-01 10:11:00\n",
      "Unique Windows: <bound method IndexOpsMixin.nunique of 0       2020-01-01 10:10:20\n",
      "1       2020-01-01 10:10:20\n",
      "2       2020-01-01 10:10:20\n",
      "3       2020-01-01 10:10:20\n",
      "4       2020-01-01 10:10:20\n",
      "                ...        \n",
      "67931   2020-01-02 09:45:20\n",
      "67932   2020-01-02 09:45:20\n",
      "67933   2020-01-02 09:45:20\n",
      "67934   2020-01-02 09:45:20\n",
      "67935   2020-01-02 09:45:30\n",
      "Name: window, Length: 67936, dtype: datetime64[ns]>\n",
      "count    8342.000000\n",
      "mean        8.143850\n",
      "std         2.389919\n",
      "min         1.000000\n",
      "25%         6.000000\n",
      "50%        10.000000\n",
      "75%        10.000000\n",
      "max        10.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "base=pd.Timestamp(\"2020-01-01\")\n",
    "us1=user_1_actigraph\n",
    "us1[\"ts\"] = base + pd.to_timedelta(us1[\"day\"] - 1, unit=\"D\") + pd.to_timedelta(us1[\"time\"]) #way to make timestamp unit D is days and default is just time\n",
    "#creates a 10s window\n",
    "us1[\"window\"]=us1[\"ts\"].dt.floor(\"10s\")\n",
    "\n",
    "print(us1[[\"day\",\"time\",\"ts\",\"window\"]].head(15))\n",
    "print(\"Unique Windows:\",us1[\"window\"].nunique)\n",
    "print(us1.groupby(\"window\").size().describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d03993e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ts              window  Activity\n",
      "0  2020-01-01 10:10:22 2020-01-01 10:10:20       2.0\n",
      "1  2020-01-01 10:10:23 2020-01-01 10:10:20       2.0\n",
      "2  2020-01-01 10:10:24 2020-01-01 10:10:20       2.0\n",
      "3  2020-01-01 10:10:25 2020-01-01 10:10:20       2.0\n",
      "4  2020-01-01 10:10:26 2020-01-01 10:10:20       2.0\n",
      "5  2020-01-01 10:10:27 2020-01-01 10:10:20       2.0\n",
      "6  2020-01-01 10:10:28 2020-01-01 10:10:20       2.0\n",
      "7  2020-01-01 10:10:29 2020-01-01 10:10:20       2.0\n",
      "8  2020-01-01 10:10:30 2020-01-01 10:10:30       2.0\n",
      "9  2020-01-01 10:10:31 2020-01-01 10:10:30       2.0\n",
      "10 2020-01-01 10:10:32 2020-01-01 10:10:30       2.0\n",
      "11 2020-01-01 10:11:01 2020-01-01 10:11:00       2.0\n",
      "12 2020-01-01 10:11:02 2020-01-01 10:11:00       2.0\n",
      "13 2020-01-01 10:11:03 2020-01-01 10:11:00       2.0\n",
      "14 2020-01-01 10:11:04 2020-01-01 10:11:00       2.0\n",
      "15 2020-01-01 10:11:05 2020-01-01 10:11:00       2.0\n",
      "16 2020-01-01 10:11:51 2020-01-01 10:11:50       2.0\n",
      "17 2020-01-01 10:11:52 2020-01-01 10:11:50       2.0\n",
      "18 2020-01-01 10:11:53 2020-01-01 10:11:50       2.0\n",
      "19 2020-01-01 10:11:54 2020-01-01 10:11:50       2.0\n",
      "Index(['Unnamed: 0', 'Axis1', 'Axis2', 'Axis3', 'Steps', 'HR',\n",
      "       'Inclinometer Off', 'Inclinometer Standing', 'Inclinometer Sitting',\n",
      "       'Inclinometer Lying', 'Vector Magnitude', 'day', 'time', 'ts', 'window',\n",
      "       'start_ts', 'end_ts', 'Activity'],\n",
      "      dtype='object')\n",
      "Labeled rows: 30352 out of 67936\n",
      "Unique activities (labeled): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "# Ensure it's sorted by time\n",
    "us1 = us1.sort_values(\"ts\").reset_index(drop=True) #sorts and drops the old index to make it non sequential\n",
    "us1_1=user_1_activity\n",
    "# Build start/end timestamps for intervals\n",
    "us1_1[\"start_ts\"] = base + pd.to_timedelta(us1_1[\"Day\"] - 1, unit=\"D\") + pd.to_timedelta(us1_1[\"Start\"] + \":00\")\n",
    "us1_1[\"end_ts\"]   = base + pd.to_timedelta(us1_1[\"Day\"] - 1, unit=\"D\") + pd.to_timedelta(us1_1[\"End\"] + \":00\")\n",
    "\n",
    "act1 = us1_1.sort_values(\"start_ts\").reset_index(drop=True)\n",
    "# Attach the most recent interval start <= ts, then drop if ts is after end_ts\n",
    "us1_labeled = pd.merge_asof(\n",
    "    us1,\n",
    "    act1[[\"start_ts\", \"end_ts\", \"Activity\"]],\n",
    "    left_on=\"ts\",\n",
    "    right_on=\"start_ts\",\n",
    "    direction=\"backward\"\n",
    ")\n",
    "us1_labeled[\"Activity\"] = np.where(\n",
    "    us1_labeled[\"end_ts\"].notna() & (us1_labeled[\"ts\"] <= us1_labeled[\"end_ts\"]),  #if condition met keep Activity if not becomes NaN\n",
    "    us1_labeled[\"Activity\"],\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# Quick checks\n",
    "print(us1_labeled[[\"ts\", \"window\", \"Activity\"]].head(20))\n",
    "print(us1_labeled.columns)\n",
    "print(\"Labeled rows:\", us1_labeled[\"Activity\"].notna().sum(), \"out of\", len(us1_labeled))\n",
    "print(\"Unique activities (labeled):\", [int(x) for x in sorted(us1_labeled[\"Activity\"].dropna().unique())[:20]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa2480f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               window  Axis1_mean  Axis1_std  Axis2_mean  Axis2_std  \\\n",
      "0 2020-01-01 10:10:20   20.375000  34.735480   12.750000  11.548036   \n",
      "1 2020-01-01 10:10:30   35.666667  25.696952   53.333333  17.243356   \n",
      "2 2020-01-01 10:11:00   43.400000  33.627370   45.000000  29.197603   \n",
      "3 2020-01-01 10:11:50   25.555556  25.239409   19.333333  24.402869   \n",
      "4 2020-01-01 10:12:00   47.800000  33.469057   98.400000  81.124322   \n",
      "\n",
      "   Axis3_mean   Axis3_std  Vector Magnitude_mean  Vector Magnitude_std  \\\n",
      "0   33.250000   31.349413              45.683750             43.143837   \n",
      "1   68.333333   76.787586              99.130000             72.752297   \n",
      "2  120.800000   59.440727             140.454000             63.122547   \n",
      "3   57.000000   54.703748              71.177778             57.754984   \n",
      "4  164.500000  125.772502             206.190000            140.164040   \n",
      "\n",
      "   Activity_win  dominance    user  \n",
      "0           2.0        1.0  user_1  \n",
      "1           2.0        1.0  user_1  \n",
      "2           2.0        1.0  user_1  \n",
      "3           2.0        1.0  user_1  \n",
      "4           2.0        1.0  user_1  \n",
      "Windows kept: 3736\n",
      "Activities: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0), np.float64(10.0)]\n",
      "count    3736.0\n",
      "mean        1.0\n",
      "std         0.0\n",
      "min         1.0\n",
      "25%         1.0\n",
      "50%         1.0\n",
      "75%         1.0\n",
      "max         1.0\n",
      "Name: dominance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "MIN_DOM=0.8 #A 10-second window is kept only if at least 80% of its accelerometer rows correspond to the same activity.\n",
    "# --- 1) ensure numeric accelerometer columns ---\n",
    "feat_cols = [\"Axis1\", \"Axis2\", \"Axis3\", \"Vector Magnitude\"]\n",
    "for c in feat_cols:\n",
    "    us1_labeled[c]=pd.to_numeric(us1_labeled[c],errors=\"coerce\")\n",
    "# --- 2) compute window-level features: mean + std ---\n",
    "X_win = us1_labeled.groupby(\"window\")[feat_cols].agg([\"mean\", \"std\"])\n",
    "X_win.columns = [f\"{a}_{b}\" for a, b in X_win.columns]\n",
    "X_win = X_win.reset_index()\n",
    "\n",
    "\n",
    "def dominant_label(s): #s is a column with activity\n",
    "    s=s.dropna()\n",
    "    if s.empty:\n",
    "        return (np.nan,0.0) #return empty list with a dominance of 0\n",
    "    vc = s.value_counts(normalize=True) #proportions if there a different activity durations\n",
    "    return (vc.index[0],float(vc.iloc[0]))\n",
    "\n",
    "y_win = us1_labeled.groupby(\"window\")[\"Activity\"].apply(dominant_label).reset_index()  #apply the dominant labels\n",
    "# print(y_win)\n",
    "y_win[[\"Activity_win\", \"dominance\"]]=pd.DataFrame(y_win[\"Activity\"].to_list(),index=y_win.index)\n",
    "y_win = y_win.drop(columns=[\"Activity\"])\n",
    "dataset_user1=X_win.merge(y_win,on=\"window\",how=\"left\") #y_win = y_win.drop(columns=[\"Activity\"])\n",
    "\n",
    "dataset_user1 = dataset_user1.dropna(subset=[\"Activity_win\"])  \n",
    "dataset_user1 = dataset_user1[dataset_user1[\"dominance\"] >= MIN_DOM].reset_index(drop=True) #drops the ones that do not have dominance\n",
    "dataset_user1[\"user\"] = \"user_1\"\n",
    "print(dataset_user1.head())\n",
    "print(\"Windows kept:\", len(dataset_user1))\n",
    "print(\"Activities:\", sorted(dataset_user1[\"Activity_win\"].unique()))\n",
    "print(dataset_user1[\"dominance\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf06ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
