{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8b2f163",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "BASE_DIR = Path(\"DataPaper\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805b1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(no):\n",
    "    user = f\"user_{no}\"    \n",
    "    user_1_actigraph = pd.read_csv(os.path.join(BASE_DIR, user, \"Actigraph.csv\"))\n",
    "    user_1_activity = pd.read_csv(os.path.join(BASE_DIR, user, \"Activity.csv\"))\n",
    "    user_1_actigraph.head()\n",
    "    # print(user_1_actigraph.columns)\n",
    "    # print(user_1_activity.columns)\n",
    "    # print(user_1_activity.head(10))\n",
    "    base=pd.Timestamp(\"2020-01-01\")\n",
    "    us1=user_1_actigraph\n",
    "    us1[\"ts\"] = base + pd.to_timedelta(us1[\"day\"] - 1, unit=\"D\") + pd.to_timedelta(us1[\"time\"]) #way to make timestamp unit D is days and default is just time\n",
    "    #creates a 10s window\n",
    "    us1[\"window\"]=us1[\"ts\"].dt.floor(\"10s\")\n",
    "\n",
    "    # print(us1[[\"day\",\"time\",\"ts\",\"window\"]].head(15))\n",
    "    # print(\"Unique Windows:\",us1[\"window\"].nunique)\n",
    "    # print(us1.groupby(\"window\").size().describe())\n",
    "    # Ensure it's sorted by time\n",
    "    us1 = us1.sort_values(\"ts\").reset_index(drop=True) #sorts and drops the old index to make it non sequential\n",
    "    us1_1=user_1_activity\n",
    "    # Build start/end timestamps for intervals\n",
    "    us1_1[\"start_ts\"] = base + pd.to_timedelta(us1_1[\"Day\"] - 1, unit=\"D\") + pd.to_timedelta(us1_1[\"Start\"] + \":00\")\n",
    "    us1_1[\"end_ts\"]   = base + pd.to_timedelta(us1_1[\"Day\"] - 1, unit=\"D\") + pd.to_timedelta(us1_1[\"End\"] + \":00\")\n",
    "\n",
    "    act1 = us1_1.sort_values(\"start_ts\").reset_index(drop=True)\n",
    "    # Attach the most recent interval start <= ts, then drop if ts is after end_ts\n",
    "    us1_labeled = pd.merge_asof(\n",
    "        us1,\n",
    "        act1[[\"start_ts\", \"end_ts\", \"Activity\"]],\n",
    "        left_on=\"ts\",\n",
    "        right_on=\"start_ts\",\n",
    "        direction=\"backward\"\n",
    "    )\n",
    "    us1_labeled[\"Activity\"] = np.where(\n",
    "        us1_labeled[\"end_ts\"].notna() & (us1_labeled[\"ts\"] <= us1_labeled[\"end_ts\"]),  #if condition met keep Activity if not becomes NaN\n",
    "        us1_labeled[\"Activity\"],\n",
    "        np.nan\n",
    "    )\n",
    "    MIN_DOM=0.8 #A 10-second window is kept only if at least 80% of its accelerometer rows correspond to the same activity.\n",
    "    # --- 1) ensure numeric accelerometer columns ---\n",
    "    feat_cols = [\"Axis1\", \"Axis2\", \"Axis3\", \"Vector Magnitude\"]\n",
    "    for c in feat_cols:\n",
    "        us1_labeled[c]=pd.to_numeric(us1_labeled[c],errors=\"coerce\")\n",
    "    # --- 2) compute window-level features: mean + std ---\n",
    "    X_win = us1_labeled.groupby(\"window\")[feat_cols].agg([\"mean\", \"std\"])\n",
    "    X_win.columns = [f\"{a}_{b}\" for a, b in X_win.columns]\n",
    "    X_win = X_win.reset_index()\n",
    "\n",
    "\n",
    "    def dominant_label(s): #s is a column with activity\n",
    "        s=s.dropna()\n",
    "        if s.empty:\n",
    "            return (np.nan,0.0) #return empty list with a dominance of 0\n",
    "        vc = s.value_counts(normalize=True) #proportions if there a different activity durations\n",
    "        return (vc.index[0],float(vc.iloc[0]))\n",
    "\n",
    "    y_win = us1_labeled.groupby(\"window\")[\"Activity\"].apply(dominant_label).reset_index()  #apply the dominant labels\n",
    "    # print(y_win)\n",
    "    y_win[[\"Activity_win\", \"dominance\"]]=pd.DataFrame(y_win[\"Activity\"].to_list(),index=y_win.index)\n",
    "    y_win = y_win.drop(columns=[\"Activity\"])\n",
    "    dataset_user1=X_win.merge(y_win,on=\"window\",how=\"left\") #y_win = y_win.drop(columns=[\"Activity\"])\n",
    "\n",
    "    dataset_user1 = dataset_user1.dropna(subset=[\"Activity_win\"])  \n",
    "    dataset_user1 = dataset_user1[dataset_user1[\"dominance\"] >= MIN_DOM].reset_index(drop=True) #drops the ones that do not have dominance\n",
    "    dataset_user1[\"user\"] = user\n",
    "    return dataset_user1\n",
    "\n",
    "all_users = []\n",
    "for i in range(1, 23):\n",
    "    try:\n",
    "        all_users.append(concatenate(i))\n",
    "    except Exception as e:\n",
    "        print(f\"User {i} failed: {e}\")\n",
    "\n",
    "dataset_all = pd.concat(all_users, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950a2a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFinal dataset:\", dataset_all.shape)\n",
    "print(\"Users included:\", dataset_all[\"user\"].nunique())\n",
    "print(\"Activities:\", sorted(dataset_all[\"Activity_win\"].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581db408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    day      time                  ts              window\n",
      "0     1  10:10:22 2020-01-01 10:10:22 2020-01-01 10:10:20\n",
      "1     1  10:10:23 2020-01-01 10:10:23 2020-01-01 10:10:20\n",
      "2     1  10:10:24 2020-01-01 10:10:24 2020-01-01 10:10:20\n",
      "3     1  10:10:25 2020-01-01 10:10:25 2020-01-01 10:10:20\n",
      "4     1  10:10:26 2020-01-01 10:10:26 2020-01-01 10:10:20\n",
      "5     1  10:10:27 2020-01-01 10:10:27 2020-01-01 10:10:20\n",
      "6     1  10:10:28 2020-01-01 10:10:28 2020-01-01 10:10:20\n",
      "7     1  10:10:29 2020-01-01 10:10:29 2020-01-01 10:10:20\n",
      "8     1  10:10:30 2020-01-01 10:10:30 2020-01-01 10:10:30\n",
      "9     1  10:10:31 2020-01-01 10:10:31 2020-01-01 10:10:30\n",
      "10    1  10:10:32 2020-01-01 10:10:32 2020-01-01 10:10:30\n",
      "11    1  10:11:01 2020-01-01 10:11:01 2020-01-01 10:11:00\n",
      "12    1  10:11:02 2020-01-01 10:11:02 2020-01-01 10:11:00\n",
      "13    1  10:11:03 2020-01-01 10:11:03 2020-01-01 10:11:00\n",
      "14    1  10:11:04 2020-01-01 10:11:04 2020-01-01 10:11:00\n",
      "Unique Windows: <bound method IndexOpsMixin.nunique of 0       2020-01-01 10:10:20\n",
      "1       2020-01-01 10:10:20\n",
      "2       2020-01-01 10:10:20\n",
      "3       2020-01-01 10:10:20\n",
      "4       2020-01-01 10:10:20\n",
      "                ...        \n",
      "67931   2020-01-02 09:45:20\n",
      "67932   2020-01-02 09:45:20\n",
      "67933   2020-01-02 09:45:20\n",
      "67934   2020-01-02 09:45:20\n",
      "67935   2020-01-02 09:45:30\n",
      "Name: window, Length: 67936, dtype: datetime64[ns]>\n",
      "count    8342.000000\n",
      "mean        8.143850\n",
      "std         2.389919\n",
      "min         1.000000\n",
      "25%         6.000000\n",
      "50%        10.000000\n",
      "75%        10.000000\n",
      "max        10.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "feature_cols = [c for c in dataset_all.columns \n",
    "                if c not in [\"window\", \"Activity_win\", \"dominance\", \"user\"]]  #X is what the model sees all the numbers and y is what the model needs to output\n",
    "\n",
    "X = dataset_all[feature_cols]\n",
    "y = dataset_all[\"Activity_win\"]\n",
    "users = dataset_all[\"user\"]\n",
    "unique_users = users.unique()\n",
    "\n",
    "train_users, test_users = train_test_split(\n",
    "    unique_users,\n",
    "    test_size=0.25,      # e.g. ~5–6 users for testing\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "train_mask = users.isin(train_users)  #checks if belongs to train or test\n",
    "test_mask  = users.isin(test_users)\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03993e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                    ts              window  Activity\n",
      "0  2020-01-01 10:10:22 2020-01-01 10:10:20       2.0\n",
      "1  2020-01-01 10:10:23 2020-01-01 10:10:20       2.0\n",
      "2  2020-01-01 10:10:24 2020-01-01 10:10:20       2.0\n",
      "3  2020-01-01 10:10:25 2020-01-01 10:10:20       2.0\n",
      "4  2020-01-01 10:10:26 2020-01-01 10:10:20       2.0\n",
      "5  2020-01-01 10:10:27 2020-01-01 10:10:20       2.0\n",
      "6  2020-01-01 10:10:28 2020-01-01 10:10:20       2.0\n",
      "7  2020-01-01 10:10:29 2020-01-01 10:10:20       2.0\n",
      "8  2020-01-01 10:10:30 2020-01-01 10:10:30       2.0\n",
      "9  2020-01-01 10:10:31 2020-01-01 10:10:30       2.0\n",
      "10 2020-01-01 10:10:32 2020-01-01 10:10:30       2.0\n",
      "11 2020-01-01 10:11:01 2020-01-01 10:11:00       2.0\n",
      "12 2020-01-01 10:11:02 2020-01-01 10:11:00       2.0\n",
      "13 2020-01-01 10:11:03 2020-01-01 10:11:00       2.0\n",
      "14 2020-01-01 10:11:04 2020-01-01 10:11:00       2.0\n",
      "15 2020-01-01 10:11:05 2020-01-01 10:11:00       2.0\n",
      "16 2020-01-01 10:11:51 2020-01-01 10:11:50       2.0\n",
      "17 2020-01-01 10:11:52 2020-01-01 10:11:50       2.0\n",
      "18 2020-01-01 10:11:53 2020-01-01 10:11:50       2.0\n",
      "19 2020-01-01 10:11:54 2020-01-01 10:11:50       2.0\n",
      "Index(['Unnamed: 0', 'Axis1', 'Axis2', 'Axis3', 'Steps', 'HR',\n",
      "       'Inclinometer Off', 'Inclinometer Standing', 'Inclinometer Sitting',\n",
      "       'Inclinometer Lying', 'Vector Magnitude', 'day', 'time', 'ts', 'window',\n",
      "       'start_ts', 'end_ts', 'Activity'],\n",
      "      dtype='object')\n",
      "Labeled rows: 30352 out of 67936\n",
      "Unique activities (labeled): [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GroupKFold, GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "\n",
    "groups=users[train_mask]\n",
    "rf=RandomForestClassifier(n_estimators=100,random_state=42,n_jobs=1)\n",
    "\n",
    "cv = GroupKFold(n_splits=5)\n",
    "param_dist = {\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_leaf\": randint(1, 15),\n",
    "    \"max_features\": [\"sqrt\", 0.5, 0.7]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=12,\n",
    "    cv=cv,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "search.fit(X_train, y_train, groups=groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2480f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               window  Axis1_mean  Axis1_std  Axis2_mean  Axis2_std  \\\n",
      "0 2020-01-01 10:10:20   20.375000  34.735480   12.750000  11.548036   \n",
      "1 2020-01-01 10:10:30   35.666667  25.696952   53.333333  17.243356   \n",
      "2 2020-01-01 10:11:00   43.400000  33.627370   45.000000  29.197603   \n",
      "3 2020-01-01 10:11:50   25.555556  25.239409   19.333333  24.402869   \n",
      "4 2020-01-01 10:12:00   47.800000  33.469057   98.400000  81.124322   \n",
      "\n",
      "   Axis3_mean   Axis3_std  Vector Magnitude_mean  Vector Magnitude_std  \\\n",
      "0   33.250000   31.349413              45.683750             43.143837   \n",
      "1   68.333333   76.787586              99.130000             72.752297   \n",
      "2  120.800000   59.440727             140.454000             63.122547   \n",
      "3   57.000000   54.703748              71.177778             57.754984   \n",
      "4  164.500000  125.772502             206.190000            140.164040   \n",
      "\n",
      "   Activity_win  dominance    user  \n",
      "0           2.0        1.0  user_1  \n",
      "1           2.0        1.0  user_1  \n",
      "2           2.0        1.0  user_1  \n",
      "3           2.0        1.0  user_1  \n",
      "4           2.0        1.0  user_1  \n",
      "Windows kept: 3736\n",
      "Activities: [np.float64(1.0), np.float64(2.0), np.float64(3.0), np.float64(4.0), np.float64(5.0), np.float64(6.0), np.float64(7.0), np.float64(8.0), np.float64(9.0), np.float64(10.0)]\n",
      "count    3736.0\n",
      "mean        1.0\n",
      "std         0.0\n",
      "min         1.0\n",
      "25%         1.0\n",
      "50%         1.0\n",
      "75%         1.0\n",
      "max         1.0\n",
      "Name: dominance, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "best_model = search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(dataset_all[\"Activity_win\"].value_counts())\n",
    "\n",
    "#Drop ultra-rare classes \n",
    "dataset_all2 = dataset_all[~dataset_all[\"Activity_win\"].isin([4.0, 10.0])].copy()\n",
    "\n",
    "#Rebuild X, y, users after dropping classes\n",
    "feature_cols = [c for c in dataset_all2.columns\n",
    "                if c not in [\"window\", \"Activity_win\", \"dominance\", \"user\"]]\n",
    "\n",
    "X = dataset_all2[feature_cols]\n",
    "y = dataset_all2[\"Activity_win\"]\n",
    "users = dataset_all2[\"user\"]\n",
    "\n",
    "#User-independent split (train/test by user)\n",
    "unique_users = users.unique()\n",
    "train_users, test_users = train_test_split(\n",
    "    unique_users, test_size=0.25, random_state=42\n",
    ")\n",
    "\n",
    "train_mask = users.isin(train_users)\n",
    "test_mask  = users.isin(test_users)\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n",
    "groups = users[train_mask]   # groups aligned to X_train rows\n",
    "\n",
    "# Base RF for tuning (keep trees smaller during search)\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=150,          # tune faster; increase later for final fit if you want\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42,\n",
    "    n_jobs=1                   # avoid nested parallelism\n",
    ")\n",
    "\n",
    "# 6) Group-aware CV and randomized search space\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_leaf\": randint(1, 15),\n",
    "    \"max_features\": [\"sqrt\", 0.5, 0.7]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=12,\n",
    "    cv=cv,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1,                 # parallelize over CV/params\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "#Fit search using groups \n",
    "search.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(\"Best CV macro-F1:\", search.best_score_)\n",
    "\n",
    "# Final test evaluation\n",
    "y_pred = best_model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b081164",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_pred).value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fbde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GroupKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.stats import randint\n",
    "\n",
    "# 1) Copy dataset and define \"rare\" classes to merge\n",
    "dataset_all2 = dataset_all.copy()\n",
    "\n",
    "rare = [1.0, 5.0, 9.0, 11.0, 12.0]   # merge these into Other\n",
    "OTHER_LABEL = 99.0\n",
    "\n",
    "dataset_all2[\"Activity_bin\"] = dataset_all2[\"Activity_win\"].where(\n",
    "    ~dataset_all2[\"Activity_win\"].isin(rare),\n",
    "    other=OTHER_LABEL\n",
    ")\n",
    "\n",
    "print(\"New label distribution:\")\n",
    "print(dataset_all2[\"Activity_bin\"].value_counts())\n",
    "\n",
    "# 2) Build X, y, users\n",
    "feature_cols = [c for c in dataset_all2.columns\n",
    "                if c not in [\"window\", \"Activity_win\", \"Activity_bin\", \"dominance\", \"user\"]]\n",
    "\n",
    "X = dataset_all2[feature_cols]\n",
    "y = dataset_all2[\"Activity_bin\"]\n",
    "users = dataset_all2[\"user\"]\n",
    "\n",
    "# 3) User-independent split by user\n",
    "unique_users = users.unique()\n",
    "train_users, test_users = train_test_split(unique_users, test_size=0.25, random_state=42)\n",
    "\n",
    "train_mask = users.isin(train_users)\n",
    "test_mask  = users.isin(test_users)\n",
    "\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n",
    "groups = users[train_mask]  # groups aligned with X_train\n",
    "\n",
    "# 4) RF + group-aware CV + randomized tuning\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=150,               # keep small during tuning\n",
    "    class_weight=\"balanced_subsample\",\n",
    "    random_state=42,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "cv = GroupKFold(n_splits=5)\n",
    "\n",
    "param_dist = {\n",
    "    \"max_depth\": [None, 10, 20, 30],\n",
    "    \"min_samples_leaf\": randint(1, 15),\n",
    "    \"max_features\": [\"sqrt\", 0.5, 0.7]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(\n",
    "    estimator=rf,\n",
    "    param_distributions=param_dist,\n",
    "    n_iter=12,\n",
    "    cv=cv,\n",
    "    scoring=\"f1_macro\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "search.fit(X_train, y_train, groups=groups)\n",
    "\n",
    "best_model = search.best_estimator_\n",
    "print(\"Best params:\", search.best_params_)\n",
    "print(\"Best CV macro-F1:\", search.best_score_)\n",
    "\n",
    "# 5) Test evaluation\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "print(\"\\nPredicted distribution:\")\n",
    "print(pd.Series(y_pred).value_counts().sort_index())\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf4e45a",
   "metadata": {},
   "source": [
    "Macro F1 score perhaps the most important recognizes ,“On average, how well does the model recognise each type of activity, regardless of how frequent it is?”. Had to merge as lower activities were not getting predicted well and hurting the score, merged into another called \"99\" here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
